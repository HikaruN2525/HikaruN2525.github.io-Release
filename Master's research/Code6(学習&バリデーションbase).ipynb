{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e240b7d2",
   "metadata": {},
   "source": [
    "## ●Notebookの内容\n",
    "\n",
    "前処理&形状変換後データ(コード5の「変数選択後の前処理&形状変換」で作成したもの)の読み込みと左右の入れ込みデータの時点ずらし、モデルの学習とバリデーション"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b54145",
   "metadata": {},
   "source": [
    "# 1. 前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbd19d",
   "metadata": {},
   "source": [
    "## 1.1 パッケージのインポート・乱数固定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f134932c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f79e8384fb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd #pandasパッケージをインポート\n",
    "import numpy as np #numpyパッケージをインポート\n",
    "import torch #ライブラリ「PyTorch」のtorchパッケージをインポート\n",
    "import torch.nn as nn #「ニューラルネットワーク」モジュールの別名定義\n",
    "import torch.nn.functional as F #「ニューラルネットワーク・活性化関数」モジュールの別名定義\n",
    "import collections\n",
    "import os\n",
    "import pickle\n",
    "import optuna\n",
    "import torch.optim as optim\n",
    "\n",
    "#乱数の固定\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32e8a7",
   "metadata": {},
   "source": [
    "## 1.2 MPSの使用指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "996ab369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPSの使用は True である(Trueなら使用可能、Falseなら使用不可)。\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print('MPSの使用は',torch.backends.mps.is_available(),'である(Trueなら使用可能、Falseなら使用不可)。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49329695",
   "metadata": {},
   "source": [
    "# 2. Early Stoppingクラスの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81420c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        #self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "        \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decreases or accuracy/f1 increases.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\")\n",
    "        #model.save_pretrained(\"\")\n",
    "        #torch.save(args, os.path.join(\"\", \"training_args.bin\"))\n",
    "        torch.save(model.to('mps').state_dict(), 'besterror_rate_min_model')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "        # # Save model checkpoint (Overwrite)\n",
    "        # if not os.path.exists(self.args.model_dir):\n",
    "        #     os.makedirs(self.args.model_dir)\n",
    "        # model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "        # model_to_save.save_pretrained(self.args.model_dir)\n",
    "\n",
    "        # # Save training arguments together with the trained model\n",
    "        # torch.save(self.args, os.path.join(self.args.model_dir, 'training_args.bin'))\n",
    "        # logger.info(\"Saving model checkpoint to %s\", self.args.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceaffa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters2(model):\n",
    "    table = PrettyTable(['Modules', 'Parameters'])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    #print(table)\n",
    "    #print(f'Total Trainable Params: {total_params}')\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cd73ec",
   "metadata": {},
   "source": [
    "# 3. 整理後データの読み込みと時点ずらし作業\n",
    "財務データ：1994/12〜2021/11\n",
    "\n",
    "配当込み収益率：1995/01〜2021/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b1bb9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pickle_in = open(\"./inright_data.pickle\",\"rb\") #ファイルの読み込み, 配当込み収益率はランク正規化をしていないデータであることに注意\n",
    "pickle_in_2 = open(\"./inleft_data.pickle\",\"rb\") #ファイルの読み込み\n",
    "pickle_in_3 = open(\"./ranked_data.pickle\",\"rb\") #ファイルの読み込み\n",
    "returns_data_4_0 = pickle.load(pickle_in)\n",
    "returns_data_4 = returns_data_4_0.iloc[1:,0:].reset_index().iloc[:,1:] #1995/1~2021/12のデータを抽出\n",
    "data_5_11_0 = pickle.load(pickle_in_2)\n",
    "data_5_11 = data_5_11_0.iloc[:324,0:].reset_index().iloc[:,1:] #1994/12~2021/11のデータを抽出\n",
    "data_5_7_0 = pickle.load(pickle_in_3)\n",
    "data_5_7_1 = data_5_7_0.query('期日.str.contains(\"1994\")', engine='python') #1994のデータを抽出\n",
    "data_5_7_2 = data_5_7_1.index.values.tolist() #1994/12のデータのindexをlist型に変更\n",
    "data_5_7 = data_5_7_0[~data_5_7_0.index.isin(data_5_7_2)] #1994/12のデータのindexにあうものを削除\n",
    "pickle_in.close()\n",
    "pickle_in_2.close()\n",
    "pickle_in_3.close()\n",
    "#display(returns_data_4), display(data_5_11), display(data_5_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc2150fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分析対象データの月数は 324 、 企業数は 1141.0 、 企業特性数は 40.0 である。\n"
     ]
    }
   ],
   "source": [
    "day_num_5 = data_5_7.期日.nunique(dropna = True) #期日の数(月数)\n",
    "com_num_5 = data_5_7.shape[0]/day_num_5 #分析可能な企業数\n",
    "chara_num = data_5_11.shape[1]/int(com_num_5) #用いる企業特性の数\n",
    "print('分析対象データの月数は',day_num_5,'、', '企業数は',com_num_5,'、', '企業特性数は',chara_num,'である。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ce051",
   "metadata": {},
   "source": [
    "# 4. 入力データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3ffd6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "右側ファクターネットワークの入力データの形状は (324, 1141) 、 左側ベータネットワークの入力データの形状は (324, 45640) である。\n"
     ]
    }
   ],
   "source": [
    "#右側ファクターネットワークの入力データ\n",
    "x = returns_data_4.to_numpy()\n",
    "#左側ベータネットワークの入力データ\n",
    "y = data_5_11.iloc[:,0:].to_numpy()\n",
    "x_2 = torch.tensor(x, dtype=torch.float32)\n",
    "y_2 = torch.tensor(y, dtype=torch.float32)\n",
    "print('右側ファクターネットワークの入力データの形状は',x.shape,'、', '左側ベータネットワークの入力データの形状は',y.shape,'である。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f1514",
   "metadata": {
    "id": "LbFcB0aBo0ou"
   },
   "source": [
    "# 5. オートエンコーダークラスの生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9875d1",
   "metadata": {},
   "source": [
    "## 5.1 CA$_{1}$アーキテクチャ(左側に第1層を追加)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d245586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factors(\n",
      "  (fc_r1): Linear(in_features=1141, out_features=5, bias=True)\n",
      "  (fc_l1): Linear(in_features=45640, out_features=32, bias=True)\n",
      "  (fc_l2): Linear(in_features=32, out_features=5705, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Factors(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Factors, self).__init__()\n",
    "        \n",
    "        #層(layer：レイヤー)を定義\n",
    "        \n",
    "        ##右側(ファクターネットワーク)の層\n",
    "        self.fc_r1 = nn.Linear( \n",
    "            N*1, #データ(特徴)の入力ユニット数(1059銘柄*1)\n",
    "            K*1) #出力ユニット数(5ファクター*1)\n",
    "\n",
    "        ##左側(ベータネットワーク)の層\n",
    "        self.fc_l1 = nn.Linear(\n",
    "            N*P, #データ(特徴)の入力ユニット数(1059銘柄*85企業特性)\n",
    "            lhidden) #出力ユニット数(32)\n",
    "        \n",
    "        self.fc_l2 = nn.Linear(\n",
    "            lhidden, #入力ユニット数(32)\n",
    "            N*K) #出力結果への出力ユニット数(1059銘柄*5ファクター)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # フォワードパスを定義\n",
    "        \n",
    "        #右側(ファクターネットワーク)のフォワードパスを定義\n",
    "        x = (self.fc_r1(x))\n",
    "        \n",
    "        #左側(ベータネットワーク)のフォワードパスを定義\n",
    "        y = F.relu(self.fc_l1(y)) #ReLU関数を適用\n",
    "        y = F.relu(self.fc_l2(y)) #ReLU関数を適用\n",
    "        \n",
    "        #サイズ自動調整\n",
    "        y = y.view(K, N) #サイズ自動調整\n",
    "        x = x.view(1, K) #サイズ自動調整\n",
    "        \n",
    "        return x, y #返り値\n",
    "\n",
    "#NNの初期値\n",
    "N = int(com_num_5) #銘柄数(1059銘柄)\n",
    "P = int(chara_num)   #企業特性数(85特性)\n",
    "K = 5    #ファクター数\n",
    "lhidden = 32  #左側隠れ層1での圧縮次元数\n",
    "\n",
    "#モデル（Factorsクラス）のインスタンス化\n",
    "model = Factors()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(model) #モデルの内容を出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2634ef75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1654487"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters2(model):\n",
    "    table = PrettyTable(['Modules', 'Parameters'])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    #print(table)\n",
    "    #print(f'Total Trainable Params: {total_params}')\n",
    "    return total_params\n",
    "count_parameters2(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb2c927",
   "metadata": {},
   "source": [
    "## 5.2 CA$_{1}$アーキテクチャ(左側に第1層を追加、P×1圧縮あり)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4626eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factors(\n",
      "  (fc_r1): Linear(in_features=1141, out_features=40, bias=True)\n",
      "  (fc_r2): Linear(in_features=40, out_features=5, bias=True)\n",
      "  (fc_l1): Linear(in_features=45640, out_features=32, bias=True)\n",
      "  (fc_l2): Linear(in_features=32, out_features=5705, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Factors(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Factors, self).__init__()\n",
    "        \n",
    "        #層(layer：レイヤー)を定義\n",
    "        \n",
    "        ##右側(ファクターネットワーク)の層\n",
    "        self.fc_r1 = nn.Linear( \n",
    "            N*1, #データ(特徴)の入力ユニット数(1059銘柄*1)\n",
    "            P*1) #出力ユニット数(93企業特性*1)\n",
    "        \n",
    "        self.fc_r2 = nn.Linear( \n",
    "            P*1, #データ(特徴)の入力ユニット数(85企業特性*1)\n",
    "            K*1) #出力ユニット数(5ファクター*1)\n",
    "\n",
    "        ##左側(ベータネットワーク)の層\n",
    "        self.fc_l1 = nn.Linear(\n",
    "            N*P, #データ(特徴)の入力ユニット数(1059銘柄*85企業特性)\n",
    "            lhidden) #出力ユニット数(32)\n",
    "        \n",
    "        self.fc_l2 = nn.Linear(\n",
    "            lhidden, #入力ユニット数(32)\n",
    "            N*K) #出力結果への出力ユニット数(1059銘柄*5ファクター)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # フォワードパスを定義\n",
    "        \n",
    "        #右側(ファクターネットワーク)のフォワードパスを定義\n",
    "        x = (self.fc_r1(x))\n",
    "        x = (self.fc_r2(x))\n",
    "        \n",
    "        #左側(ベータネットワーク)のフォワードパスを定義\n",
    "        \n",
    "        y = F.relu(self.fc_l1(y)) #ReLU関数を適用\n",
    "        y = F.relu(self.fc_l2(y)) #ReLU関数を適用\n",
    "        \n",
    "        #サイズ自動調整\n",
    "        \n",
    "        y = y.view(K, N) #サイズ自動調整\n",
    "        x = x.view(1, K) #サイズ自動調整\n",
    "        \n",
    "        return x, y #返り値\n",
    "\n",
    "#NNの初期値\n",
    "N = int(com_num_5) #銘柄数(1059銘柄)\n",
    "P = int(chara_num)   #企業特性数(85特性)\n",
    "K = 5    #ファクター数\n",
    "lhidden = 32  #左側隠れ層1での圧縮次元数\n",
    "\n",
    "#モデル（Factorsクラス）のインスタンス化\n",
    "model = Factors()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(model) #モデルの内容を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c3c8c",
   "metadata": {},
   "source": [
    "# 6. 学習とバリデーション"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af311c77",
   "metadata": {},
   "source": [
    "## 6.1 学習とバリデーション関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0125492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 1\n",
    "num_month   = 1\n",
    "x_batch = num_month\n",
    "y_batch = num_month\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#学習の関数\n",
    "def train(model, optimizer, alpha, device):\n",
    "    model.train()\n",
    "    #for i_year in range(18):\n",
    "    for i_year in range(2):\n",
    "        xtrain_loader = torch.utils.data.DataLoader(x_2[:((i_year+1)*12),:], batch_size=x_batch, num_workers=num_workers, shuffle=False, pin_memory=True)\n",
    "        ytrain_loader = torch.utils.data.DataLoader(y_2[:((i_year+1)*12),:], batch_size=y_batch, num_workers=num_workers, shuffle=False, pin_memory=True)\n",
    "        for x_input, y_input in zip(xtrain_loader, ytrain_loader):\n",
    "            x_input = x_input.to(device)\n",
    "            y_input = y_input.to(device)\n",
    "            u ,v = model(x_input.float(), y_input.float()) #モデルの出力を取得(左側出力v、右側出力uとする)\n",
    "            uv = torch.mm(u, v) #最終的に積をとる\n",
    "            loss = criterion(uv, x_input.float()) #入力x.float()と復元outputsの誤差を取得\n",
    "            \n",
    "            # パラメータのL1ノルムを損失関数に足す\n",
    "            #l1 = sum(torch.norm(w , 1) for w in model.parameters()) #リスト内包表記\n",
    "            l1 = sum(torch.norm(w/count_parameters2(model) , 1)  for w in model.parameters()) #リスト内包表記\n",
    "            #loss = loss + alpha*l1.clone().detach()\n",
    "            #loss = loss + alpha*l1.detach().clone()\n",
    "            loss = loss + alpha*l1.detach()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    #print(x_input, y_input)\n",
    "    #print('ここらで一区切りです')\n",
    "\n",
    "#バリデーションの関数\n",
    "def valid(model, alpha, device, xvalid_loader, yvalid_loader):\n",
    "    model.eval() #model.train(mode=False)でも良い\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_input, y_input in zip(xvalid_loader, yvalid_loader):\n",
    "            x_input = x_input.to(device)\n",
    "            y_input = y_input.to(device)\n",
    "            u ,v = model(x_input.float(), y_input.float()) #モデルの出力を取得(左側出力v、右側出力uとする)\n",
    "            uv = torch.mm(u, v) #最終的に積をとる\n",
    "            loss = criterion(uv, x_input.float()) #入力x.float()と復元outputsの誤差を取得\n",
    "            \n",
    "            valid_loss += loss.item()  # 誤差(損失)の更新\n",
    "            \n",
    "        #1エポックあたりの損失を求める\n",
    "        valid_loss = valid_loss/len(xvalid_loader)\n",
    "\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd6848c",
   "metadata": {},
   "source": [
    "## 6.2 最適化関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06c7b5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#最適化関数(specify loss function)をAdamに設定\n",
    "def get_optimizer(trial, model):\n",
    "    adam_lr = trial.suggest_loguniform('adam_lr', 1e-10, 1e-1) #学習率を変化させる\n",
    "    #weight_decay = trial.suggest_loguniform('weight_decay', 1e-10, 1e-3)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=adam_lr, weight_decay=0) #weight_decayは0\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd43d37",
   "metadata": {},
   "source": [
    "## 6.3 目的関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25bb2088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# エポック数\n",
    "n_epochs = 5\n",
    "error_rate_min = 0.0\n",
    "def objective(trial):\n",
    "    global i_trial\n",
    "    print('今は', i_trial ,'回目のトライアル')\n",
    "    optimizer = get_optimizer(trial, model)\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-10, 1e-1)\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True) #ここでインスタンス化\n",
    "    #学習の実行\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        print('Epoch: {}'.format(epoch))\n",
    "        train_loss = 0.0\n",
    "        train(model, optimizer, alpha, device)\n",
    "        error_rate = valid(model, alpha, device, xvalid_loader, yvalid_loader)\n",
    "        \n",
    "        early_stopping(error_rate, model) # 最良モデルならモデルパラメータ保存\n",
    "        if early_stopping.early_stop: \n",
    "            # 一定epochだけval_lossが最低値を更新しなかった場合、ここに入り学習を終了\n",
    "            break\n",
    "    \n",
    "    global error_rate_min\n",
    "    if i_trial == 0:\n",
    "        #error_rate_min = error_rate\n",
    "        error_rate_min = early_stopping.val_loss_min\n",
    "        model.load_state_dict(torch.load('besterror_rate_min_model'))\n",
    "        torch.save(model.to('mps').state_dict(), 'error_rate_min_model')\n",
    "        #print(list(model.parameters())[0]) #確認用\n",
    "    else:\n",
    "        if error_rate < error_rate_min:\n",
    "            #error_rate_min = error_rate\n",
    "            error_rate_min = early_stopping.val_loss_min\n",
    "            model.load_state_dict(torch.load('besterror_rate_min_model'))\n",
    "            torch.save(model.to('mps').state_dict(), 'error_rate_min_model')\n",
    "            print('model updated')\n",
    "            #print(list(model.parameters())[0]) #確認用\n",
    "    i_trial += 1\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b74654e",
   "metadata": {},
   "source": [
    "## 6.4 学習とバリデーション(パラメータチューニング)の実行\n",
    "ハイパーパラメーターは学習率とl1の2つ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d1e1938",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-15 18:01:39,815]\u001b[0m A new study created in memory with name: no-name-c5faf2d6-c051-4622-868a-4ab11c0f9b22\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今は 0 回目のトライアル\n",
      "Epoch: 1\n",
      "Validation loss decreased (inf --> 0.009402).  Saving model ...\n",
      "Epoch: 2\n",
      "Validation loss decreased (0.009402 --> 0.009170).  Saving model ...\n",
      "Epoch: 3\n",
      "Validation loss decreased (0.009170 --> 0.008951).  Saving model ...\n",
      "Epoch: 4\n",
      "Validation loss decreased (0.008951 --> 0.008730).  Saving model ...\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-15 18:01:55,909]\u001b[0m Trial 0 finished with value: 0.00854461468406953 and parameters: {'adam_lr': 8.696040132105582e-06, 'alpha': 0.00027334069690310554}. Best is trial 0 with value: 0.00854461468406953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.008730 --> 0.008545).  Saving model ...\n",
      "今は 1 回目のトライアル\n",
      "Epoch: 1\n",
      "Validation loss decreased (inf --> 0.008107).  Saving model ...\n",
      "Epoch: 2\n",
      "Validation loss decreased (0.008107 --> 0.007840).  Saving model ...\n",
      "Epoch: 3\n",
      "Validation loss decreased (0.007840 --> 0.007600).  Saving model ...\n",
      "Epoch: 4\n",
      "Validation loss decreased (0.007600 --> 0.007472).  Saving model ...\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-15 18:02:11,381]\u001b[0m Trial 1 finished with value: 0.0074058735626749694 and parameters: {'adam_lr': 2.6599310838681845e-05, 'alpha': 8.015832747965139e-06}. Best is trial 1 with value: 0.0074058735626749694.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.007472 --> 0.007406).  Saving model ...\n",
      "model updated\n",
      "今は 2 回目のトライアル\n",
      "Epoch: 1\n",
      "Validation loss decreased (inf --> 0.007404).  Saving model ...\n",
      "Epoch: 2\n",
      "Validation loss decreased (0.007404 --> 0.007402).  Saving model ...\n",
      "Epoch: 3\n",
      "Validation loss decreased (0.007402 --> 0.007400).  Saving model ...\n",
      "Epoch: 4\n",
      "Validation loss decreased (0.007400 --> 0.007398).  Saving model ...\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-15 18:02:26,893]\u001b[0m Trial 2 finished with value: 0.007396543747745454 and parameters: {'adam_lr': 6.499698237449664e-07, 'alpha': 6.50200078509766e-05}. Best is trial 2 with value: 0.007396543747745454.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.007398 --> 0.007397).  Saving model ...\n",
      "model updated\n"
     ]
    }
   ],
   "source": [
    "TRIAL_SIZE = 3\n",
    "i_year_2 = 4 #バリデーション期間の年数\n",
    "xvalid_loader = torch.utils.data.DataLoader(x_2[216:216+(i_year_2*12),:], batch_size=x_batch, num_workers=num_workers, shuffle=False ,pin_memory=True)\n",
    "yvalid_loader = torch.utils.data.DataLoader(y_2[216:216+(i_year_2*12),:], batch_size=y_batch, num_workers=num_workers, shuffle=False ,pin_memory=True)\n",
    "\n",
    "i_trial = 0\n",
    "study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=0)) #123\n",
    "study.optimize(objective, n_trials=TRIAL_SIZE)\n",
    "#最適化したハイパーパラメータの結果\n",
    "best_params = study.best_params\n",
    "f = open('best_params.txt', 'w')\n",
    "f.write(str(best_params))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b1274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
